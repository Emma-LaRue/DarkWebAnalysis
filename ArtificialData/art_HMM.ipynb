{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import compress\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data (from pickle file)\n",
    "import pickle\n",
    "file = open('art_data.pkl','rb')\n",
    "data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('art_train.pkl','rb')\n",
    "train = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('art_test.pkl','rb')\n",
    "test = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get subset of data for HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of train/test sequences to use\n",
    "n_train = 10\n",
    "n_test = 2\n",
    "len_seq = 100 # length of each sequence\n",
    "\n",
    "# Get subset of train/test set\n",
    "v_train = np.array(train['vendor_name'][-n_train*len_seq:])\n",
    "d_train = np.array(train['drug'][-n_train*len_seq:])\n",
    "v_test = np.array(test['vendor_name'][:n_test*len_seq])\n",
    "d_test = np.array(test['drug'][:n_test*len_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get probability for a variable given another variable\n",
    "def get_cond_probDist(category, var, given_var):\n",
    "#     Function gets probability distribution of 'var' given 'given_var' is 'category'\n",
    "#     Args: 'category' is the class of given_var\n",
    "#           'var' is the random variable for which the prob. dist. is computed\n",
    "#           'given_var' is the random variable which is given\n",
    "#     Returns: series, representing proportion of total for each drug\n",
    "    subset = train[train[given_var] == category]\n",
    "    tally = subset[given_var].groupby(subset[var]).count()\n",
    "    return(tally/np.sum(tally))\n",
    "\n",
    "# Create dictionary to predict using Bayes' Rule (i.e. using conditional probability)\n",
    "def makeEmissionProbTable(var = 'drug', given_var = 'vendor_name'):\n",
    "#     Function returns a dictionary with sorted probabilities for 'var' given 'given_var'\n",
    "#     Args: 'var' is the name of the variable to predict\n",
    "#           'given_var' is the name of the given variable\n",
    "#     Returns: dictionary with keys for each possible outcome of 'given_var', and values corresponding to \n",
    "#                 sorted probabilities for each outcome of 'var'\n",
    "    \n",
    "    # Get list of unique vendors and of unique drugs\n",
    "    cols = train[var].unique()\n",
    "    rows = train[given_var].unique()\n",
    "\n",
    "    #Create conditional probability dataframe\n",
    "    cond_prob_df = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    #Insert given variable name column\n",
    "    cond_prob_df.insert(0, given_var, rows)\n",
    "    \n",
    "    # Apply function to dataframe\n",
    "    cond_prob_df.iloc[:,1:] = \\\n",
    "        cond_prob_df[given_var].apply(get_cond_probDist, var = var, given_var = given_var)\n",
    "    \n",
    "    # fill NA values with 0\n",
    "    cond_prob_df = cond_prob_df.fillna(0)\n",
    "    \n",
    "    # Set index to be given variable\n",
    "    cond_prob_df = cond_prob_df.set_index(given_var)\n",
    "  \n",
    "    return(cond_prob_df)\n",
    "\n",
    "# Create emission probability table\n",
    "ems_prob_table = makeEmissionProbTable()\n",
    "ems_prob = np.array(ems_prob_table, dtype=np.float) # convert to float\n",
    "ems_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to encode and decode the sequences\n",
    "def encode(seq, encoder):\n",
    "    code = np.array([encoder[element] for element in seq])\n",
    "    return code\n",
    "\n",
    "def decode(code, codebook):\n",
    "    seq = [codebook[element] for element in code]\n",
    "    return seq\n",
    "\n",
    "# Encode drugs for training\n",
    "drug_categories = list(ems_prob_table)\n",
    "drug_keys = range(0, len(drug_categories))\n",
    "drug_encoder = dict(zip(drug_categories, drug_keys))\n",
    "drug_codebook = dict(zip(drug_keys, drug_categories))\n",
    "d_train_encoded = encode(d_train, drug_encoder)\n",
    "d_test_encoded = encode(d_test, drug_encoder)\n",
    "\n",
    "# Make vendor codebook (for evaluation)\n",
    "vendor_list = list(ems_prob_table.index)\n",
    "vendor_keys = range(0, len(vendor_list))\n",
    "vendor_codebook = dict(zip(vendor_keys, vendor_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for model\n",
    "v_train = v_train.reshape((n_train,len_seq))\n",
    "d_train_encoded = d_train_encoded.reshape((n_train,len_seq))\n",
    "v_test = v_test.reshape((n_test,len_seq))\n",
    "d_test_encoded = d_test_encoded.reshape((n_test,len_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Baum-Welch算法来估计参数，然后使用Viterbi算法来预测隐状态数列。\n",
    "\n",
    "def hmm_BaumWelch(data, n_states, n_iter=100, n_train=10):\n",
    "    \"\"\"\n",
    "    @ parameters:\n",
    "        data (matrix): The encoded observed sequence of hidden state\n",
    "        n_states (int): The length of sequence for hidden state. \n",
    "        n_iter (int): The iteration of hmm.MultinomialHMM.\n",
    "        n_train (int): The training times.\n",
    "    @ return:\n",
    "        start_prob (list): Start probability with size = 1 x n_states.\n",
    "        trans_prob (matrix): Transition probability matrix with size = n_states x n_states.\n",
    "    \"\"\"\n",
    "    # Check the input format\n",
    "    if len(data) < 2 or len(data[0]) < 1:\n",
    "        return print(\"The input dataset should be 2D.\")\n",
    "    if n_states < 1 or n_iter < 1 or n_train < 1:\n",
    "        return print(\"Please input positive integer number for parameters.\")\n",
    "    # Initialize the parameters\n",
    "    score0 = float(\"-inf\")\n",
    "    start_prob = 0\n",
    "    trans_prob = 0\n",
    "    # Build the model\n",
    "    model = hmm.MultinomialHMM(n_components=n_states, n_iter=int(n_iter))\n",
    "    # Run \"n_train\" times to train the model and select the best parameters.\n",
    "    for _ in range(0, int(n_train)):\n",
    "        model.fit(data) # 模型的拟合比较耗时\n",
    "        print(\"Model fitted in round {0}!\".format(_))\n",
    "        # hmm2.0里的score函数接受2D作为输入格式\n",
    "        scores = sum([model.score([data[i]]) for i in range(len(data))]) #score越大越好（若带负号，则绝对值越小越好）\n",
    "        print(scores)\n",
    "        if scores > score0:\n",
    "            start_prob = model.startprob_\n",
    "            trans_prob = model.transmat_  \n",
    "            # 发射概率已知，这里无须估计。\n",
    "        score0 = max(score0, scores)\n",
    "    return start_prob, trans_prob\n",
    "\n",
    "# https://stackoverflow.com/questions/34379911/how-to-run-hidden-markov-models-in-python-with-hmmlearn\n",
    "def hmm_Viterbi(obs_seq, start_prob, trans_prob, ems_prob, n_states, n_train=10, verbose=True):\n",
    "    \"\"\"\n",
    "    @ parameters:\n",
    "        obs_seq (list): An encoded observed sequence of hidden state.\n",
    "        start_prob (list): Start probability with size = 1 x n_states.\n",
    "        trans_prob (matrix): Transition probability matrix with size = n_states x n_states.\n",
    "        ems_prob (matrix): Emission probability with size = len(vlist) x len(dlist).\n",
    "        n_states (int): The length of sequence for hidden state. \n",
    "        n_train (int): The training times.\n",
    "        verbose (boolean): Print out the comment if True.\n",
    "    @ return:\n",
    "        vendor0 (list): Predicted sequence of hidden state.\n",
    "    \"\"\"\n",
    "    # Check the input format\n",
    "    if len(obs_seq) < 1:\n",
    "        return print(\"The length of observed sequence should be larger than 1.\")\n",
    "    if n_states < 1 or n_train < 1:\n",
    "        return print(\"Please input positive integer number for parameters.\")\n",
    "    if n_states != (len(trans_prob) or len(trans_prob[0])):\n",
    "        return print(\"The size of trans_prob is not correct.\")\n",
    "    if len(start_prob) != len(trans_prob):\n",
    "        return print(\"The size of start_prob and trans_prob doesn't match.\")    \n",
    "    # Build the model and initialize the parameters\n",
    "    model = hmm.MultinomialHMM(n_components=n_states)\n",
    "    model.startprob_= start_prob\n",
    "    model.transmat_ = trans_prob\n",
    "    model.emissionprob_ = ems_prob\n",
    "    logprob0 = float(\"-inf\")\n",
    "    vendor0 = 0\n",
    "    # Run \"n_train\" times to train the model and select the best parameters.\n",
    "    for _ in range(0, int(n_train)):\n",
    "        # decode: Given parameters and observed sequence, employ Viterbi algorithm to predict the hidden state.\n",
    "        logprob, vendors = model.decode(list(obs_seq), algorithm=\"viterbi\") # obs_seq must be list rather than array.\n",
    "        if logprob > logprob0: # update \"vendors\" when logprob is larger than previous one.\n",
    "            vendor0 = vendors\n",
    "        logprob0 = max(logprob0, logprob)\n",
    "    if verbose:\n",
    "        print(\"The best logprob is: \", logprob0) #该参数反映模型拟合的好坏,数值越大越好。\n",
    "    return vendor0\n",
    "\n",
    "def evaluation(hs_seq_pred, hs_seq_gt, verbose=True):\n",
    "    # 评价预测结果：Compute the accuracy\n",
    "    if verbose:\n",
    "        print(\"The set of predicted hidden state sequence: \\n\", set(hs_seq_pred))\n",
    "        print(\"-\"*90)\n",
    "        print(\"The set of groundtruth hidden state sequence: \\n\", set(hs_seq_gt))\n",
    "    cnt = 0\n",
    "    for i in range(len(hs_seq_pred)):\n",
    "        if hs_seq_pred[i] == hs_seq_gt[i]:\n",
    "            cnt += 1\n",
    "    return round(cnt/len(hs_seq_pred), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Baum-Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted in round 0!\n",
      "-2488.919551977607\n",
      "Training_time: 7.11 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "data_dseq_ec = d_train_encoded\n",
    "\n",
    "start_time = time.time()\n",
    "# Employ Baum-Welch algorithm to estimate the parameters\n",
    "start_prob, trans_prob = hmm_BaumWelch(d_train_encoded, len(vendor_list), n_iter=1, n_train=1)\n",
    "end_time = time.time()\n",
    "print('Training_time:',round(end_time-start_time,2),'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save parameters to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('start_prob_art'+str(datetime.now()),'wb')\n",
    "pickle.dump(start_prob, file)\n",
    "file.close()\n",
    "\n",
    "file = open('trans_prob_art'+str(datetime.now()),'wb')\n",
    "pickle.dump(trans_prob, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode hidden sequence using Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best logprob is:  -513.2853111062138\n",
      "The best logprob is:  -511.64556354949536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n",
      "/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n",
      "/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n",
      "/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n",
      "/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n",
      "/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n",
      "/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n",
      "/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n",
      "/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n",
      "/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n"
     ]
    }
   ],
   "source": [
    "# 测试：预测和评价\n",
    "def testing(test_obs, test_hs, idx, drug_codebook, vendor_codebook, verbose=True):\n",
    "    obs_seq = test_obs[idx]\n",
    "    obs_seq_ec = obs_seq.reshape(-1,1)\n",
    "\n",
    "    # Employ Viterbi algorithm to estimate the parameters\n",
    "    vendors = hmm_Viterbi(obs_seq_ec, start_prob, trans_prob, ems_prob, len(vendor_list), n_train=5, verbose = verbose)\n",
    "    \n",
    "    # Evaluation（对预测结果进行评价-accuracy）\n",
    "    hs_seq_pred = [vendor_list[vendor_idx] for vendor_idx in vendors]\n",
    "    hs_seq_gt = test_hs[idx]\n",
    "    acc = evaluation(hs_seq_pred, hs_seq_gt, verbose=False)\n",
    "    \n",
    "    actual_drugs = decode(obs_seq, drug_codebook)\n",
    "    actual_vendors = hs_seq_gt\n",
    "    predicted_vendors = decode(vendors, vendor_codebook)\n",
    "    \n",
    "    df = pd.DataFrame({'drug':actual_drugs, 'actual_vendor': actual_vendors, 'predicted_vendor':predicted_vendors})\n",
    "    df['correct'] = df['actual_vendor'] == df['predicted_vendor']\n",
    "\n",
    "    return(df)\n",
    "\n",
    "# Scenario3: select all samples from the testing set.\n",
    "results = pd.concat([testing(d_test_encoded, v_test, idx, drug_codebook, vendor_codebook)\\\n",
    "                for idx in range(len(d_test_encoded))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "20\n",
      "Accuracy: 8.0 %\n"
     ]
    }
   ],
   "source": [
    "print(len(results.actual_vendor.value_counts()))\n",
    "print(len(results.predicted_vendor.value_counts()))\n",
    "\n",
    "n_correct = results.correct.value_counts()[True]\n",
    "n_wrong = results.correct.value_counts()[False]\n",
    "accuracy = n_correct/(n_correct+n_wrong)\n",
    "print('Accuracy:',round(100*accuracy,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-06-26 18:18:08.703152'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
